{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports and requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 14:33:19.572863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-25 14:33:19.701491: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-25 14:33:19.736599: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-07-25 14:33:21.204037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-25 14:33:21.790774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10000 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:65:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, layers as L\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "# let's add the root folder, it contains all the necessary useful functions for data processing\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The basic neural network solution provides all data processing and all stages of preprocessing. In this notebook, we will omit this section and use ready-made data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../val_buckets/processed_chunk_000.pkl',\n",
       " '../../../val_buckets/processed_chunk_001.pkl',\n",
       " '../../../val_buckets/processed_chunk_002.pkl',\n",
       " '../../../val_buckets/processed_chunk_003.pkl',\n",
       " '../../../val_buckets/processed_chunk_004.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset = '../../../val_buckets'\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_val = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../train_buckets/processed_chunk_000.pkl',\n",
       " '../../../train_buckets/processed_chunk_001.pkl',\n",
       " '../../../train_buckets/processed_chunk_002.pkl',\n",
       " '../../../train_buckets/processed_chunk_003.pkl',\n",
       " '../../../train_buckets/processed_chunk_004.pkl',\n",
       " '../../../train_buckets/processed_chunk_005.pkl',\n",
       " '../../../train_buckets/processed_chunk_006.pkl',\n",
       " '../../../train_buckets/processed_chunk_007.pkl',\n",
       " '../../../train_buckets/processed_chunk_008.pkl',\n",
       " '../../../train_buckets/processed_chunk_009.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset = '../../../train_buckets'\n",
    "dir_with_datasets = os.listdir(path_to_dataset)\n",
    "dataset_train = sorted([os.path.join(path_to_dataset, x) for x in dir_with_datasets])\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_generators import batches_generator, transaction_features\n",
    "from tf_training import train_epoch, eval_model, inference\n",
    "from training_aux import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['currency',\n",
       " 'operation_kind',\n",
       " 'card_type',\n",
       " 'operation_type',\n",
       " 'operation_type_group',\n",
       " 'ecommerce_flag',\n",
       " 'payment_system',\n",
       " 'income_flag',\n",
       " 'mcc',\n",
       " 'country',\n",
       " 'city',\n",
       " 'mcc_category',\n",
       " 'day_of_week',\n",
       " 'hour',\n",
       " 'weekofyear',\n",
       " 'amnt',\n",
       " 'days_before',\n",
       " 'hour_diff']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All features in our model will be categorical. To represent them in the model, we use categorical embeddings. To do this, you need to set the dimension of the latent space for each categorical feature. We use [formula](https://forums.fast.ai/t/sizeof-embedding-for-categorical-variables/42608 ) from the library `fast.ai `. All mappings are stored in the `embedding_projections.pkl` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../constants/embedding_projections.pkl', 'rb') as f:\n",
    "    embedding_projections = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implementing the model. We will present all input features in the form of embeddings, we will configure them to get a vector representation of the transaction. We use SpatialDropout to regularize embeddings. Let's imagine the `product` attribute as a separate embedding.\n",
    "Let's feed the sequences into the `BiGRU` recurrent network and the `Conv1D` convolutional neural network. \n",
    "For a recurrent network, we use all the hidden states of the network to get an aggregated view of the transaction history - we skip all the hidden states of `BiGRU` through `AvgPooling` and through `MaxPooling'.\n",
    "For a convolutional network, we will skip all embedding through 3 different branches of the convolutional network with different parameters, for each branch we will apply `GlobalMaxPooling'.\n",
    "Concatenate all the results. Based on this input, we will build a small `MLP` that acts as a classifier for the target task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transactions_rnn(transactions_cat_features, embedding_projections, product_col_name='product', \n",
    "                          rnn_units=128, classifier_units=32, optimizer=None):\n",
    "    if not optimizer:\n",
    "        optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "        \n",
    "    inputs = []\n",
    "    cat_embeds = []\n",
    "    # Categorical features embedding layers\n",
    "    for feature_name in transactions_cat_features:\n",
    "        inp = L.Input(shape=(None, ), dtype='uint32', name=f'input_{feature_name}')\n",
    "        inputs.append(inp)\n",
    "        source_size, projection = embedding_projections[feature_name]\n",
    "        emb = L.Embedding(source_size+1, projection, trainable=True, mask_zero=False, name=f'embedding_{feature_name}')(inp)\n",
    "        cat_embeds.append(emb)\n",
    "    \n",
    "    # product feature\n",
    "    inp = L.Input(shape=(1, ), dtype='uint32', name=f'input_product')\n",
    "    inputs.append(inp)\n",
    "    source_size, projection = embedding_projections['product']\n",
    "    product_emb = L.Embedding(source_size+1, projection, trainable=True, mask_zero=False, name=f'embedding_product')(inp)\n",
    "    product_emb_reshape = L.Reshape((projection, ))(product_emb)\n",
    "    \n",
    "    concated_cat_embeds = L.concatenate(cat_embeds)\n",
    "    dropout_embeds = L.SpatialDropout1D(0.1)(concated_cat_embeds)\n",
    "    # RNN\n",
    "    sequences = L.Bidirectional(L.GRU(units=rnn_units, return_sequences=True))(dropout_embeds)\n",
    "    pooled_avg_sequences = L.GlobalAveragePooling1D()(sequences)\n",
    "    pooled_max_sequences = L.GlobalMaxPooling1D()(sequences)\n",
    "    \n",
    "    # Convolutoion NN\n",
    "    # Convolution layers architecture\n",
    "    n_conv_1 = n_conv_2 = n_conv_3 = 256 \n",
    "    k_conv_1 = 3\n",
    "    k_conv_2 = 2\n",
    "    k_conv_3 = 4\n",
    "    # Conv layers\n",
    "    conv_1 = L.Conv1D(n_conv_1, k_conv_1, activation='relu', name='conv_1')(dropout_embeds) \n",
    "    conv_1_1 = L.Conv1D(n_conv_1, k_conv_1, activation='relu', name='conv_1_1')(conv_1)\n",
    "    maxp_1 = L.GlobalMaxPooling1D(name='maxp_1')(conv_1)\n",
    "\n",
    "    conv_2 = L.Conv1D(n_conv_2, k_conv_2, activation='relu', name='conv_2')(dropout_embeds)\n",
    "    conv_2_1 = L.Conv1D(n_conv_2, k_conv_2, activation='relu', name='conv_2_1')(conv_2)\n",
    "    maxp_2 = L.GlobalMaxPooling1D(name='maxp_2')(conv_2)\n",
    "\n",
    "    conv_3 = L.Conv1D(n_conv_3, k_conv_3, activation='relu', name='conv_3')(dropout_embeds)\n",
    "    conv_3_1 = L.Conv1D(n_conv_3, k_conv_3, activation='relu', name='conv_3_1')(conv_3)\n",
    "    maxp_3 = L.GlobalMaxPooling1D(name='maxp_3')(conv_3)\n",
    "\n",
    "    # concatenate RNN and CNN\n",
    "    concated = L.concatenate([pooled_avg_sequences, pooled_max_sequences, product_emb_reshape, maxp_1, maxp_2, maxp_3])\n",
    "    # MLP\n",
    "    dense_intermediate = L.Dense(classifier_units, activation='relu', \n",
    "                                 kernel_regularizer=keras.regularizers.L1L2(1e-7, 1e-5))(concated)\n",
    "    proba = L.Dense(1, activation='sigmoid')(dense_intermediate)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=proba)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../../rnn_baseline/checkpoints/tf_advanced_baseline\n",
    "! mkdir ../../rnn_baseline/checkpoints/tf_advanced_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to detect retraining, we use Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoints = '../../rnn_baseline/models/commit_2/'\n",
    "es = EarlyStopping(patience=3, mode='max', verbose=True, save_path=os.path.join(path_to_checkpoints, 'best_checkpoint.pt'), \n",
    "                   metric_name='ROC-AUC', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_transactions_rnn(transaction_features, embedding_projections, classifier_units=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's start the training cycle, we will log the loss every epoch, as well as the roc-auc for validation and training. We will save the weights after each epoch, as well as the best ones using early_stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_batch_size = 128\n",
    "val_batch_szie = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "1080/7270 [===>..........................] - ETA: 13:53 - loss: 0.1391"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_histories =[]\n",
    "best_roc_auc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    training_history = train_epoch(model, dataset_train, batch_size=train_batch_size, shuffle=True, cur_epoch=epoch, \n",
    "                steps_per_epoch=7270) #default steps_per_epoch=7270\n",
    "    training_histories.append(training_history)\n",
    "    \n",
    "    val_roc_auc = eval_model(model, dataset_val, batch_size=val_batch_szie)\n",
    "    if val_roc_auc>best_roc_auc:\n",
    "        best_roc_auc = val_roc_auc\n",
    "        model.save_weights(os.path.join(path_to_checkpoints, f'epoch_{epoch+1}_val_{val_roc_auc:.3f}.hdf5'))\n",
    "        print(f'Saved model with val_roc_auc: {val_roc_auc}')\n",
    "        train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_szie)\n",
    "    print(f'Epoch {epoch+1} completed. Val roc-auc: {val_roc_auc}')\n",
    "    \n",
    "    es(val_roc_auc, model)\n",
    "    \n",
    "    if es.early_stop:\n",
    "        print('Early stopping reached. Stop training...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model with val_roc_auc: 0.7948809484933049\n"
     ]
    }
   ],
   "source": [
    "print(f'Saved model with val_roc_auc: {best_roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Train roc-auc: 0.7986922694571151, Val roc-auc: 0.7801614959015882\n"
     ]
    }
   ],
   "source": [
    "print(f'Epoch {epoch+1} completed. Train roc-auc: {train_roc_auc}, Val roc-auc: {val_roc_auc}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train roc-auc: 0.89001976660625, Val roc-auc: 0.7916524842879937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Everything is ready to make predictions for the test sample. You only need to prepare the data in the same format as for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1063620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1063622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1063623</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1063624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    app_id  product\n",
       "0  1063620        0\n",
       "1  1063621        0\n",
       "2  1063622        1\n",
       "3  1063623        1\n",
       "4  1063624        2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frame = pd.read_csv('../../../test_target_contest.csv')\n",
    "test_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../test_buckets/processed_chunk_000.pkl',\n",
       " '../../../test_buckets/processed_chunk_001.pkl',\n",
       " '../../../test_buckets/processed_chunk_002.pkl',\n",
       " '../../../test_buckets/processed_chunk_003.pkl',\n",
       " '../../../test_buckets/processed_chunk_004.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_test_dataset = '../../../test_buckets/'\n",
    "dir_with_test_datasets = os.listdir(path_to_test_dataset)\n",
    "dataset_test = sorted([os.path.join(path_to_test_dataset, x) for x in dir_with_test_datasets])\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A separate question is which of the constructed models to use in order to make predictions for the test. You can choose the best one by early_stopping. In this case, there is a risk that we will fit the validation sample, especially if it is not very representative, but this is the most basic option (we use it). You can make different versions of the ensembling using weights from different eras. This approach requires additional code. Finally, you can choose a model that shows good results on validation and at the same time is not too retrained for train sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_checkpoint.pt.data-00000-of-00001\tepoch_2_val_0.779.hdf5\n",
      "best_checkpoint.pt.index\t\tepoch_3_val_0.774.hdf5\n",
      "checkpoint\t\t\t\tepoch_3_val_0.783.hdf5\n",
      "epoch_1_val_0.500.hdf5\t\t\tepoch_4_val_0.777.hdf5\n",
      "epoch_1_val_0.563.hdf5\t\t\tepoch_4_val_0.792.hdf5\n",
      "epoch_1_val_0.757.hdf5\t\t\tepoch_5_val_0.780.hdf5\n",
      "epoch_1_val_0.764.hdf5\t\t\tepoch_5_val_0.794.hdf5\n",
      "epoch_1_val_0.774.hdf5\t\t\tepoch_6_val_0.795.hdf5\n",
      "epoch_1_val_0.776.hdf5\t\t\tepoch_7_val_0.796.hdf5\n",
      "epoch_2_val_0.770.hdf5\t\t\tepoch_8_val_0.800.hdf5\n"
     ]
    }
   ],
   "source": [
    "! ls $path_to_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_transactions_rnn(transaction_features, embedding_projections, classifier_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fb449a2ba30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_checkpoints = '../../rnn_baseline/checkpoints/tf_advanced_baseline/'\n",
    "model.load_weights(os.path.join(path_to_checkpoints, 'best_checkpoint.pt'))\n",
    "\n",
    "test_preds = inference(model, dataset_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1063655</td>\n",
       "      <td>0.014757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1063672</td>\n",
       "      <td>0.033401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1063694</td>\n",
       "      <td>0.007221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1063709</td>\n",
       "      <td>0.036303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1063715</td>\n",
       "      <td>0.020206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    app_id     score\n",
       "0  1063655  0.014757\n",
       "1  1063672  0.033401\n",
       "2  1063694  0.007221\n",
       "3  1063709  0.036303\n",
       "4  1063715  0.020206"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd497a2224f342dd92a2f27b254e2ee938aba9d762855c84936c6d0324f4bd2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
